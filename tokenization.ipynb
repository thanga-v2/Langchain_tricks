{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPxAm1PbMAc0mUmJWT37kI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanga-v2/Langchain_tricks/blob/main/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wmbLAW5r7aU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwemodel = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen3-1.7B\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZcHbgeJ-sZ19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")"
      ],
      "metadata": {
        "id": "DLQawslfsyLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to BCCI Commitee for the tragic covid mishap during the IPL Explain how it happened.<|assistant|>\""
      ],
      "metadata": {
        "id": "Dz2XsWCht-hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id = tokenizer(prompt,\n",
        "                     return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "y1YT4p-JuHt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(input_id))\n",
        "print(input_id)"
      ],
      "metadata": {
        "id": "SU7AtDh_uU7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation\n",
        "output = qwemodel.generate(\n",
        "    input_ids = input_id[\"input_ids\"], # Corrected line\n",
        "    max_new_tokens = 200\n",
        ")"
      ],
      "metadata": {
        "id": "J8P81ausuWaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "xfi6siImvSQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "ug5VkkWTvHgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizers\n",
        "\n",
        "# what's inside ?\n",
        "\n",
        "print(input_id['input_ids'])"
      ],
      "metadata": {
        "id": "NOmZQq34vKJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in input_id['input_ids']:\n",
        "  print(tokenizer.decode(id))"
      ],
      "metadata": {
        "id": "Ghr72ixewSrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id in output[0]:\n",
        "  print(tokenizer.decode(id))"
      ],
      "metadata": {
        "id": "_4mPjPcbwnVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how tokenizer breaks down the text ?\n",
        "\n",
        "# 1. creator of the model can descide on his own -> BPE(Byte pair encoding) for GPT and BERT\n",
        "\n",
        "# 2. design the choices like voccabulary size.\n",
        "\n",
        "# 3.the tokenizer needs to be trained on specific dataset.\n",
        "\n",
        "# tokenizer trained on english dataset will differ from the tokenizer trained on multilingual dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2bP9wbAw0Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer breaks down text to word tokens, subword tokens, character tokens, bytes."
      ],
      "metadata": {
        "id": "gWi0YEMayZj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"\"\"\n",
        "English and CAPITALIZATION\n",
        "ðŸŽµé¸Ÿ\n",
        "\" \"\n",
        "show_tokens False None elif == >= else: two tabs:\" \" Three tabs:\n",
        "12.0*50=600\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6NPfkqatyvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors_list = [\n",
        "'102;194;165', '252;141;98', '141;160;203',\n",
        "'231;138;195', '166;216;84', '255;217;47'\n",
        "]"
      ],
      "metadata": {
        "id": "RgWBtxzry9HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT\n",
        "\n",
        "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
      ],
      "metadata": {
        "id": "DbDQXARby_aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmasker(\"Hello I'm a [MASK] model.\")"
      ],
      "metadata": {
        "id": "hE6YYy-vzp4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT 2 -> neural machine transaltion of rare words with sub words\n",
        "\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "generator(text, max_length=100, num_return_sequences=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "wmjsioBUz9DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "HHA2ZxsS0x63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testtokens = tokenizer(\"Hi i am test data to see how i am being represented in embeddings\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "EK7SOW2m3HxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testtokens['input_ids'][0])"
      ],
      "metadata": {
        "id": "RJsEapLy4EQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testoutput = qwemodel(**testtokens)[0]"
      ],
      "metadata": {
        "id": "VNsq6_d53a2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(testoutput)"
      ],
      "metadata": {
        "id": "MJKJ4vpu3reh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testoutput.shape"
      ],
      "metadata": {
        "id": "wMNvahyX3vLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in testtokens['input_ids'][0]:\n",
        "  print(tokenizer.decode(token))"
      ],
      "metadata": {
        "id": "loGGtitm3_nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELUh6x4q4M-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}