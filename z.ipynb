{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=os.getenv('OPENAI_API_KEY')\n",
    "os.environ['GROQ_API_KEY']=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user is asking me to \"brief your arch,\" which I think they mean my architecture. Let me start by recalling what I know about my own setup. I remember that I\\'m a large language model developed by Alibaba Cloud, specifically part of the Qwen series. My architecture is based on a transformer model, which is a common architecture for these kinds of models. \\n\\nI should mention the key components of a transformer model: attention mechanisms, which allow me to focus on different parts of the input when generating a response. Also, the encoder-decoder structure, maybe? Wait, actually, some models use an autoregressive approach where the decoder is used for generation. Let me make sure. I think I\\'m an autoregressive model, which means I generate text one token at a time, using the previous tokens to predict the next one.\\n\\nThe user might also be interested in the scale of the model. I know that large language models can have billions of parameters. I should check if I can mention the parameter count, but sometimes that\\'s kept confidential or not disclosed exactly. I remember in some presentations, they mentioned scale-out design, which means the model can be scaled across multiple GPUs efficiently. That\\'s part of the architecture too, dealing with distributed computing.\\n\\nTraining data is another aspect. My training data comes from Alibaba Group\\'s internal historical accumulation up to December 2024. The dataset is massive, which helps in learning a wide range of topics and languages. I should mention the training data cutoff date because that\\'s important for the user to know the knowledge is up to that point.\\n\\nMultilingual support is another feature. I support multiple languages, so I should list some of them or just say \"over 100 languages\" or similar. Also, capabilities like context length and the ability to handle long documents are important. I think I have a maximum context length of 32,768 tokens, which is quite large, allowing for detailed interactions.\\n\\nApplications like answering questions, writing stories, emails, scripts, logical reasoning, coding, etc., are part of my capabilities. The user might want to know what I can do, so mentioning those use cases would be helpful.\\n\\nI should structure this in a clear way, starting with the architecture type (transformer-based), then key components like attention mechanisms, autoregressive approach, parameter count, training data, multilingual support, context length, and use cases. I should also mention the scale-out design for scalability.\\n\\nWait, but I need to be careful not to include any proprietary information. The user just wants a brief overview. Let me avoid technical jargon as much as possible, but still be precise. Maybe start with a summary sentence first, then break down the components.\\n\\nHmm, also, I should check if there\\'s anything else in my training data or architecture that\\'s unique. The scale-out training framework is a key point because it allows training very large models efficiently. And maybe the fact that I\\'m optimized for both speed and efficiency, allowing me to handle real-time interactions.\\n\\nLet me put this all together in a coherent way without getting too technical. The user probably isn\\'t an expert, so simplicity is key. Make sure to highlight the main points: transformer-based architecture, autoregressive, large parameter count, extensive training data, multilingual, long context length, and the various tasks I can perform. Also mention the training framework for scalability if it\\'s a notable feature.\\n\\nWait, but I\\'m supposed to be concise. Let me structure it as bullet points or a short paragraph. Since the user said \"brief,\" a paragraph should suffice. Start with the main architecture, then key features, training data, capabilities, and maybe a note on performance.\\n</think>\\n\\nCertainly! Here\\'s a brief overview of my architecture and key features:\\n\\n- **Transformer-Based**: I\\'m built on a transformer architecture, which uses self-attention mechanisms to weigh the relevance of input tokens, enabling me to understand context and relationships in text effectively.\\n\\n- **Autoregressive Design**: I generate text token-by-token, using prior tokens to predict the next one, which allows for coherent and context-aware responses.\\n\\n- **Large-Scale Parameters**: I have a massive number of parameters (though exact count is confidential), contributing to my ability to handle complex tasks like coding, reasoning, and creative writing.\\n\\n- **Training Data**: My training data comes from Alibaba Group\\'s internal historical data up to December 2024, covering a vast range of topics and languages.\\n\\n- **Multilingual Support**: I support over 100 languages, making me versatile for international users.\\n\\n- **Long Context Length**: I can process and generate extremely long texts (up to 32,768 tokens), ideal for detailed documents, code, or multi-step reasoning.\\n\\n- **Applications**: I excel at answering questions, writing stories, emails, scripts, performing logical reasoning, coding (including handling multiple programming languages), and more.\\n\\n- **Scalability**: My architecture is optimized for distributed training and inference, allowing efficient scaling across hardware for high performance.\\n\\nThis design balances speed, efficiency, and versatility, enabling me to assist with a wide array of tasks while maintaining real-time responsiveness. Let me know if you\\'d like deeper details on any specific aspect! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1076, 'prompt_tokens': 19, 'total_tokens': 1095, 'completion_time': 2.651646558, 'prompt_time': 0.00405382, 'queue_time': 0.42139164, 'total_time': 2.655700378}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--5f0141e1-06f9-4f32-ab24-4c370ee117e1-0', usage_metadata={'input_tokens': 19, 'output_tokens': 1076, 'total_tokens': 1095})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hello friend, can you brief your arch ?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
