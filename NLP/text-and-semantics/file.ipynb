{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words 2 vec as machines can't understand words but are good at manipulating numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thangaraj/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the first and last 15% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thangaraj/miniconda3/lib/python3.12/importlib/__init__.py:90: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "Generating train split: 100%|██████████| 8530/8530 [00:00<00:00, 323707.19 examples/s]\n",
      "Generating validation split: 100%|██████████| 1066/1066 [00:00<00:00, 477837.78 examples/s]\n",
      "Generating test split: 100%|██████████| 1066/1066 [00:00<00:00, 586993.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_datset = load_dataset(\"rotten_tomatoes\", \n",
    "                            split = \"train[:15%]+train[-15%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"rotten_tomatoes\",\n",
    "                            split=\"test[:15%]+test[-15%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "print(len(train_datset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class POS_Vectorizer that has the method vectorize to process the text and the text is turned into a vector of size 10.\n",
    "\n",
    "it process the text and count number of \n",
    "\n",
    "verbs\n",
    "\n",
    "proper nouns\n",
    "\n",
    "adjectives \n",
    "\n",
    "adverbs\n",
    "\n",
    "aux verbs\n",
    "\n",
    "pronouns\n",
    "\n",
    "numbers\n",
    "\n",
    "punctuation etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_vectorizer:\n",
    "    def __init__(self, spacy_model):\n",
    "        self.model = spacy_model\n",
    "\n",
    "    def vectorize(self, input_text):\n",
    "            doc = self.model(input_text)\n",
    "            vector = []\n",
    "            vector.append(len(doc))\n",
    "            pos = {\"VERB\":0, \"NOUN\":0, \"PROPN\":0, \"ADJ\":0, \"ADV\":0, \"AUX\":0, \"PRON\":0, \"NUM\":0, \"PUNCT\":0}\n",
    "\n",
    "            for token in doc:\n",
    "                if token.pos_ in pos:\n",
    "                    pos[token.pos_] += 1\n",
    "            vector_values = list(pos.values())\n",
    "            vector = vector + vector_values\n",
    "            return vector        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560\n",
      "{'text': 'a sober and affecting chronicle of the leveling effect of loss .', 'label': 1}\n",
      "{'text': 'a fast , funny , highly enjoyable movie .', 'label': 1}\n",
      "{'text': \"writer/director joe carnahan's grimy crime drama is a manual of precinct cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic .\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_datset))\n",
    "print(train_datset[1086])\n",
    "print(train_datset[1087])\n",
    "print(train_datset[1089])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_datset[1089][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = POS_vectorizer(small_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.vectorize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer/director joe carnahan's grimy crime drama is a manual of precinct cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic .\n",
      "[31, 2, 9, 2, 3, 2, 1, 2, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)\n",
    "print(vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
