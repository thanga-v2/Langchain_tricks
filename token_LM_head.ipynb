{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNY0vrCsbEX2hNL0w5L1BQO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanga-v2/Langchain_tricks/blob/main/token_LM_head.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upeY3ysTtJa-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwemodel = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen3-1.7B\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "7ToNdMFxttqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qwemodel)"
      ],
      "metadata": {
        "id": "9QvbBoM3tvrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(embed_tokens): Embedding(151936, 2048) -> 151936 unique tokens\n",
        "\n",
        "tokens are not words. its actually sub words.\n",
        "\n",
        "[\"Playing\", \"##ing\", \"with\", \"Qwen\"] → 4 tokens.\n",
        "\n",
        "what is 3b, 2b, 7b etc ?\n",
        "\n",
        "if a model has 15 tokens in a sentences, the model is trained over 200 million sentence datasets, then its\n",
        "\n",
        "200 million * 15 -> 3+ billion\n",
        "\n",
        "better way to say is,\n",
        "\n",
        "the model is trained over 3 billion token pieces of dataset.\n",
        "\n",
        "vocab_size = how many unique words/subwords the model can understand\n",
        "\n",
        "training_tokens = how many total pieces it read during training"
      ],
      "metadata": {
        "id": "0OrnzfeCxoPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding Strategy -\n",
        "\n",
        "\n",
        "The method of choosing a\n",
        "single token from the probability distribution is called\n",
        "the decoding strategy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Nd_s_9Uyyw_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **The easiest decoding strategy would be to always pick the\n",
        "token with the highest probability score.**"
      ],
      "metadata": {
        "id": "s_ux004ozNQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")"
      ],
      "metadata": {
        "id": "SeAijlEyzRVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "-F5E7LIU0Lyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwentokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")"
      ],
      "metadata": {
        "id": "7AB_yQP00PnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qwentokenizer)"
      ],
      "metadata": {
        "id": "h0-ZFzo40WBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Hi I am thanga from Chennai. May I know how are you ?\"\n"
      ],
      "metadata": {
        "id": "w9vJyuvX0cU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_id = tokenizer.tokenize(prompt)"
      ],
      "metadata": {
        "id": "A3za7cQT012m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_id)"
      ],
      "metadata": {
        "id": "cojEb4Bg08x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(bert_id))"
      ],
      "metadata": {
        "id": "wKIe0UM11OeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_id = tokenizer.tokenize(prompt,\n",
        "                             return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "MxDyjh9U1lI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_id)"
      ],
      "metadata": {
        "id": "XXo8F_um1rWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_id = qwentokenizer.tokenize(prompt)"
      ],
      "metadata": {
        "id": "Z0fGyACf0-5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qwen_id)"
      ],
      "metadata": {
        "id": "Wi78yLSU1Ce-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(qwen_id))"
      ],
      "metadata": {
        "id": "ckRxcGj41RgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_id = qwentokenizer.tokenize(prompt,\n",
        "                                 return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "xjNejB6d2R8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qwen_id)"
      ],
      "metadata": {
        "id": "N1FS5ZhM2XOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you are using the qwentokenizer and its tensor output\n",
        "qwen_id_tensor = qwentokenizer.tokenize(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "QLGS9iCj3Hcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwentokenizer.tokenize(\"thanga\", return_tensors = \"pt\")"
      ],
      "metadata": {
        "id": "VFm836Tv30uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "qwentokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
        "\n",
        "# ✅ Proper usage:\n",
        "encoded = qwentokenizer(\"thanga\", return_tensors=\"pt\")\n",
        "print(encoded[\"input_ids\"])"
      ],
      "metadata": {
        "id": "ZKArDf5w487Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "id": "Q8UAVypx4_T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "berttokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")"
      ],
      "metadata": {
        "id": "xiWGijiC7Kfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = berttokenizer(\"thanga\", return_tensors=\"pt\")\n",
        "print(encoded[\"input_ids\"])"
      ],
      "metadata": {
        "id": "o70xGdop7RFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "id": "dyCdcjKr7Yi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Gw5dU2Q7ift"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHycyVxo7icZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNW7zlLz7iYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nYmsQz4_7iTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U18jvioX7iPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qwemodel)"
      ],
      "metadata": {
        "id": "KF4y_47O7iAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"The capital of india is\""
      ],
      "metadata": {
        "id": "ceKNXolP7kV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = qwentokenizer(prompt, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "nWOhrMWi7oGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input)\n",
        "print(input['input_ids'].shape)"
      ],
      "metadata": {
        "id": "aCUAGeN97tOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the output of the model before the lm_head"
      ],
      "metadata": {
        "id": "nTomzWLP76qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = qwemodel.model(input['input_ids'])"
      ],
      "metadata": {
        "id": "BVycDMbl8CEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_output)"
      ],
      "metadata": {
        "id": "RxJPUSJG8Sok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_output[0])"
      ],
      "metadata": {
        "id": "Pd5AAUb18dvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output = qwemodel.lm_head(model_output[0])"
      ],
      "metadata": {
        "id": "MylOkusu8YkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm_head_output[0])"
      ],
      "metadata": {
        "id": "tTFvmZJI8hjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ],
      "metadata": {
        "id": "giZ4gVp9_gku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "id": "q0_Fi_gEAux2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}