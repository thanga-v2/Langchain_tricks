{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi My name is Thanga.\" I should respond politely. Let me start with a greeting. Maybe \"Hello Thanga!\" to acknowledge their name. Then I can introduce myself as Qwen. I should offer assistance by asking how I can help them today. Keep it friendly and open-ended. Let me check for any typos. Looks good. Alright, send that response.\\n</think>\\n\\nHello Thanga! I\\'m Qwen, a large language model developed by Alibaba Cloud. How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 16, 'total_tokens': 128, 'completion_time': 0.262623319, 'prompt_time': 0.005067305, 'queue_time': 0.666473812, 'total_time': 0.267690624}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--c8089c39-5ac7-432e-a469-0cee3ad950a1-0', usage_metadata={'input_tokens': 16, 'output_tokens': 112, 'total_tokens': 128})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\")\n",
    "model.invoke(\"Hi My name is Thanga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_groq.chat_models.ChatGroq'>\n"
     ]
    }
   ],
   "source": [
    "print(ChatGroq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"Qwen-Qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x1091e1220> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x1091e05f0> model_name='Qwen-Qwq-32b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user asked, \"Hi who are you?\" I need to respond politely and clearly explain who I am. Let me start by greeting them back. Then mention my name, Qwen, and explain that I\\'m a large language model developed by Alibaba Cloud. I should highlight my capabilities, like answering questions, creating text, and assisting with various tasks. It\\'s important to mention that I can help with things like writing stories, emails, scripts, and more. Also, I should invite them to ask me anything or give them a task. Let me make sure the tone is friendly and approachable. Keep it concise but informative. Let me put that all together in a natural way without any markdown formatting.\\n</think>\\n\\nHello! I\\'m Qwen, a large language model developed by Alibaba Cloud. I\\'m here to assist you with any questions, tasks, or creative ideas you might have. Whether you need help writing a story, drafting an email, solving a problem, or just want to chat, feel free to ask me anything! How can I assist you today? ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 220, 'prompt_tokens': 15, 'total_tokens': 235, 'completion_time': 0.506528477, 'prompt_time': 0.003798736, 'queue_time': 0.632086002, 'total_time': 0.510327213}, 'model_name': 'Qwen-Qwq-32b', 'system_fingerprint': 'fp_a91d9c2cfb', 'finish_reason': 'stop', 'logprobs': None}, id='run--1dde0684-9e1a-48cd-a734-f3d2d84cc198-0', usage_metadata={'input_tokens': 15, 'output_tokens': 220, 'total_tokens': 235})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user introduced themselves as Thanga. I should respond politely and maybe ask how I can assist them today. Let me make sure to keep it friendly and open-ended so they feel comfortable to ask for help with whatever they need. Maybe something like, \"Hello Thanga! How can I assist you today?\" That should work.\\n</think>\\n\\nHello Thanga! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! ðŸ˜Š', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 15, 'total_tokens': 118, 'completion_time': 0.252858729, 'prompt_time': 0.002922439, 'queue_time': 2.880743359, 'total_time': 0.255781168}, 'model_name': 'Qwen-Qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--9278b086-64a9-4104-8614-a2507b15299b-0', usage_metadata={'input_tokens': 15, 'output_tokens': 103, 'total_tokens': 118})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi I am thanga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n<think>\\nThe user is asking about my caching mechanism. I should guide him to check Qwen's official website.\\n</think>\\n\\nFor more information about me, please visit Qwen's official website.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 17, 'total_tokens': 55, 'completion_time': 0.092260801, 'prompt_time': 0.003824405, 'queue_time': 3.404106373, 'total_time': 0.096085206}, 'model_name': 'Qwen-Qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None}, id='run--9534d982-ad14-408b-8813-f3f87476a8e2-0', usage_metadata={'input_tokens': 17, 'output_tokens': 38, 'total_tokens': 55})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Do you have cache mechanism ?  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI expert, I can certainly tell you about nuclear fusion! \n",
      "\n",
      "**What is Nuclear Fusion?**\n",
      "\n",
      "Nuclear fusion is the process of combining two light atomic nuclei to form a heavier nucleus, releasing a tremendous amount of energy in the process. It's the same reaction that powers the sun and other stars.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Energy Release:** Fusion releases far more energy than nuclear fission (splitting atoms), which is the process used in current nuclear power plants.\n",
      "* **Fuel Source:** Fusion typically uses readily available isotopes of hydrogen, like deuterium and tritium, which are found in seawater.\n",
      "* **Clean Energy:** Fusion produces no long-lived radioactive waste, making it a potentially cleaner energy source than fission.\n",
      "* **Safety:** Fusion reactions are inherently safe because they cannot sustain themselves without carefully controlled conditions.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "* **Extreme Conditions:** Fusion requires extremely high temperatures (millions of degrees Celsius) and pressures to overcome the electrostatic repulsion between the positively charged nuclei.\n",
      "* **Confinement:** Containing the superheated plasma (a state of matter where electrons are stripped from atoms) is a major technical challenge. Two main approaches are magnetic confinement (using powerful magnets) and inertial confinement (using high-powered lasers).\n",
      "* **Sustaining the Reaction:** Achieving a sustained fusion reaction, where the energy produced exceeds the energy input, is a key scientific and engineering hurdle.\n",
      "\n",
      "**Current Research:**\n",
      "\n",
      "* **ITER:** The International Thermonuclear Experimental Reactor (ITER) is a massive international collaboration aiming to demonstrate the feasibility of fusion power.\n",
      "* **National Ignition Facility (NIF):** In the United States, the NIF uses lasers to achieve inertial confinement fusion.\n",
      "\n",
      "**Future Potential:**\n",
      "\n",
      "If successful, fusion power has the potential to provide a safe, clean, and nearly limitless source of energy for the future.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about nuclear fusion!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Nuclear fusion\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Hi I am thanga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain= prompt|model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Prompt Engineering\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
