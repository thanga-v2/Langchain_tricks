{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic AI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x10943e120> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x10943ee40> model_name='qwen-qwq-32b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"Hi am a new one !\" which is a friendly greeting. First, I should respond politely. Let me check if there\\'s any typo or misunderstanding. The user might have meant \"Hi, I\\'m a new one!\" so maybe a typo with the apostrophe. I should acknowledge their greeting and offer help. Let me make sure to ask them how I can assist. Keep it simple and welcoming. Maybe say something like \"Hello! Welcome! How can I assist you today?\" or add an emoji to keep it friendly. Also, maybe they\\'re new to using chatbots, so keep the tone approachable. Alright, that should work.\\n</think>\\n\\nHello! Welcome! ðŸ˜Š How can I assist you today? Don\\'t hesitate to ask anythingâ€”whether you\\'re curious about something, need help with a task, or just want to chat!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 177, 'prompt_tokens': 16, 'total_tokens': 193, 'completion_time': 0.431057277, 'prompt_time': 0.003147089, 'queue_time': 0.413515939, 'total_time': 0.434204366}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_512a3da6bb', 'finish_reason': 'stop', 'logprobs': None}, id='run--5d711b73-fffc-40c6-9a5b-68b8eb896e6d-0', usage_metadata={'input_tokens': 16, 'output_tokens': 177, 'total_tokens': 193})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hi am a new one !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"\\n<think>\\nThe user is asking about my context length. I should guide him to check Qwen's official website.\\n</think>\\n\\nFor more information about me, please visit Qwen's official website.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 25, 'total_tokens': 63, 'completion_time': 0.092239181, 'prompt_time': 0.007432124, 'queue_time': 0.42025328300000003, 'total_time': 0.099671305}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None} id='run--5ce89dc4-82f4-45b3-b698-f6ce842bc8c9-0' usage_metadata={'input_tokens': 25, 'output_tokens': 38, 'total_tokens': 63}\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(\"what is your contect length as of today? can you process beyond?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\n<think>\\nOkay, the user is asking how to find their LM head details. Hmm, first I need to figure out what LM head details they\\'re referring to. LM could stand for Language Model, so maybe they\\'re asking about the parameters or architecture of the large language model they\\'re using. \\n\\nWait, the user might be using a framework like Hugging Face\\'s transformers, where models have heads like the LM head for language modeling tasks. So they probably want to know how to access the details of the model\\'s head. \\n\\nFirst step, I should consider different frameworks. If they\\'re using Hugging Face, the process is different from something like PyTorch or TensorFlow. Let me recall: in Hugging Face, models often have a base model and then a head on top, like the LM head for masked language modeling. \\n\\nSo to get the LM head details, they might need to access the model\\'s architecture. For example, in HuggingFace\\'s transformers library, when you load a model, say from AutoModelForMaskedLM, the model has a base model (like BertModel) and a prediction head (BertOnlyMLMHead). \\n\\nSo the steps would be: load the model, then access the last layer or the specific head. Maybe they can print the model\\'s structure to see the layers. \\n\\nWait, the user might not be a programmer, but perhaps they\\'re working with a specific tool or API. Alternatively, maybe they\\'re referring to the hardware\\'s LM head, but that seems less likely. \\n\\nAssuming it\\'s about the model architecture, I should outline the steps for a few common frameworks. For PyTorch, you can print the model\\'s layers. For Hugging Face, they can inspect the model\\'s components. \\n\\nI should also mention that the LM head is usually the layer responsible for the final output, like the linear layer in BERT. So checking the model\\'s parameters or structure would show the head\\'s details. \\n\\nPossible example code snippets would help. Also, maybe mention using utilities like model.config to see configuration parameters. \\n\\nWait, the user might be confused between the base model and the head. So explaining the difference first could be useful. \\n\\nI should also consider that if they have a saved model, they might need to load it and inspect the layers. Or if they\\'re using a pre-trained model from a library, how to access those details. \\n\\nAnother point: the LM head might have specific attributes like the number of parameters, input/output dimensions. So the steps would involve accessing that part of the model and checking its parameters. \\n\\nI need to make sure the instructions are clear and cover different scenarios, like if they\\'re using Hugging Face\\'s transformers, or a custom PyTorch model. \\n\\nAlso, maybe they need to use a utility function to summarize the model structure, like model.summary() in some frameworks. \\n\\nWait, in PyTorch, printing the model with print(model) would show the structure, allowing them to identify the LM head layer. \\n\\nAnother angle: the LM head is part of the model\\'s architecture, so showing how to locate it in the model\\'s layer list. \\n\\nI should also mention that different architectures have different names for the head. For example, in BERT, it\\'s the \"cls\" layer, while in GPT, it\\'s the final linear layer. \\n\\nIncluding examples for both encoder-decoder and decoder-only models might be necessary. \\n\\nAlso, checking the model\\'s configuration. The config might have parameters like vocab size, hidden size, which are part of the head\\'s details. \\n\\nI need to structure the answer step-by-step. Start with understanding what LM head refers to, then the steps in different frameworks, and examples. \\n\\nWait, maybe they\\'re using a specific model and just need to know where to find the head in their code. So the answer should guide them through inspecting the model\\'s structure programmatically. \\n\\nI should also caution that the exact method depends on the model and framework, so providing general steps with examples is better. \\n\\nIncluding code examples for Hugging Face and PyTorch would be helpful. Maybe also TensorFlow if applicable. \\n\\nMake sure to explain terms like base model vs. head, so the user understands the context. \\n\\nAlright, putting it all together: start with an explanation, then steps for different frameworks, code examples, and notes on configuration files. \\n\\nAlso, mention that the LM head is typically the last layer, so looking for the final linear layer or specific named modules. \\n\\nI should check if there are standard methods in common libraries for accessing the head. For example, in Hugging Face\\'s BERT, the cls.predictions module is the head. \\n\\nSo in code, they might do something like model.cls.predictions... \\n\\nWait, in recent versions, Hugging Face\\'s models might have a \"get_output_embeddings()\" method or similar. \\n\\nHmm, maybe better to just show how to print the model\\'s structure and look for the head\\'s name, then access its parameters. \\n\\nAlso, note that the head\\'s parameters include weights and biases, which can be inspected via .weight or .bias attributes if it\\'s a linear layer. \\n\\nIncluding examples for both BERT and GPT-style models would be good. \\n\\nOkay, I think I have a structure. Start by explaining what the LM head is, then provide steps for different frameworks, including code examples, and mention checking the configuration files or model parameters. \\n\\nMake sure the instructions are clear for someone with basic programming knowledge in PyTorch/Hugging Face.\\n</think>\\n\\nTo find the details of your **LM (Language Model) head** in a transformer-based model (e.g., BERT, GPT, etc.), you need to inspect the model\\'s architecture or configuration. The LM head is typically the final layer used for tasks like language modeling, token prediction, or classification. Here\\'s how to do this in popular frameworks:\\n\\n---\\n\\n### **1. Using Hugging Face\\'s Transformers Library (PyTorch/TF)**\\n#### **Step 1: Load the Model**\\nFirst, load your model using Hugging Face\\'s `transformers` library:\\n```python\\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\\n\\nmodel_name = \"bert-base-uncased\"\\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n```\\n\\n#### **Step 2: Inspect the Model Structure**\\nPrint the model\\'s architecture to identify the LM head layer(s):\\n```python\\nprint(model)\\n```\\n\\nThis will output the model\\'s layers. For example, in BERT, the LM head is usually called `cls.predictions` (for masked language modeling). In other models like GPT-2, it might be named `lm_head`.\\n\\n#### **Step 3: Access the LM Head Directly**\\nUse the model\\'s attribute names to access the LM head:\\n```python\\n# Example for BERT\\'s LM head (masked LM)\\nlm_head = model.cls.predictions.decoder  # Or model.lm_head for some models\\nprint(\"LM Head Details:\")\\nprint(lm_head)\\n```\\n\\n#### **Step 4: Get Layer Dimensions and Parameters**\\nCheck the input/output dimensions and parameters of the LM head:\\n```python\\nprint(\"Input size:\", lm_head.in_features)  # Input dimension\\nprint(\"Output size:\", lm_head.out_features)  # Output dimension (usually vocabulary size)\\nprint(\"Parameters:\", sum(p.numel() for p in lm_head.parameters()))  # Number of parameters\\n```\\n\\n---\\n\\n### **2. Using PyTorch (Without Hugging Face)**\\nIf you\\'re working with a custom PyTorch model:\\n```python\\n# Example: A simple LM head (linear layer)\\nclass LMHeadModel(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.base_model = ...  # Your base model\\n        self.lm_head = nn.Linear(768, vocab_size)  # Example LM head\\n\\n    def forward(...):\\n        ...\\n\\nmodel = LMHeadModel()\\n```\\n\\nTo inspect the LM head:\\n```python\\nprint(model.lm_head)  # Shows the layer details (e.g., in_features, out_features)\\nprint(\"Input size:\", model.lm_head.in_features)\\nprint(\"Output size:\", model.lm_head.out_features)\\n```\\n\\n---\\n\\n### **3. Using TensorFlow**\\nIn TensorFlow/Keras:\\n```python\\n# Example: Accessing the LM head in a loaded model\\nmodel = ...  # Your loaded model\\nlm_head_layer = model.get_layer(\"lm_head\")  # Replace \"lm_head\" with the actual layer name\\nprint(lm_head_layer.summary())  # Shows layer details\\n```\\n\\n---\\n\\n### **4. Check the Model Configuration**\\nModel configurations often specify the LM head details:\\n```python\\nfrom transformers import AutoConfig\\n\\nconfig = AutoConfig.from_pretrained(model_name)\\nprint(config)  # Look for `hidden_size`, `vocab_size`, and other LM head-related parameters\\n```\\n\\n---\\n\\n### **Key Properties to Look For**\\n- **Input/Output Dimensions**: The LM head typically takes the base model\\'s output (e.g., hidden state embeddings) and maps it to the vocabulary size.\\n- **Activation Function**: Some heads apply a softmax or ReLU after the linear layer.\\n- **Layer Type**: The head is usually a linear (dense) layer, but it might include other components like embeddings or bias terms.\\n\\n---\\n\\n### **Example: BERT\\'s LM Head Structure**\\nIn BERT (masked LM), the LM head typically has:\\n1. A linear layer to project the hidden states to the hidden dimension.\\n2. A bias layer.\\n3. A final linear layer to project to the vocabulary size.\\n\\nExample output from `print(model)`:\\n```python\\n...\\n(cls): BertOnlyMLMHead(\\n  (predictions): BertLMPredictionHead(\\n    (transform): BertPredictionHeadTransform(\\n      (dense): Linear(in_features=768, out_features=768, bias=True)\\n      (LayerNorm): LayerNorm(...)\\n    )\\n    (decoder): Linear(in_features=768, out_features=30522, bias=True)\\n  )\\n)\\n...\\n```\\n\\nHere, the final `decoder` layer (`Linear(in=768, out=30522)`) is the LM head, with `out_features` matching the vocabulary size (e.g., 30,522 for BERT).\\n\\n---\\n\\n### **Common Names for LM Heads in Popular Models**\\n- **BERT**: `cls.predictions.decoder`\\n- **GPT-2**: `lm_head` (a linear layer)\\n- **RoBERTa**: Similar to BERT (`lm_head` in some versions).\\n- **T5**: The LM head is part of the decoder stack.\\n\\n---\\n\\n### **Troubleshooting**\\n- If the head isnâ€™t visible, check the modelâ€™s documentation or configuration.\\n- Use `model.named_children()` to list all submodules and find the head by name:\\n  ```python\\n  for name, module in model.named_children():\\n      print(name)  # Look for \"lm_head\", \"cls\", or similar names\\n  ```\\n\\nBy following these steps, you can identify and analyze the LM headâ€™s architecture and parameters.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 2323, 'prompt_tokens': 18, 'total_tokens': 2341, 'completion_time': 5.781835301, 'prompt_time': 0.003828267, 'queue_time': 0.295282382, 'total_time': 5.785663568}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None} id='run--a83f3be3-b7a7-49b6-bc77-153fb17d9ce3-0' usage_metadata={'input_tokens': 18, 'output_tokens': 2323, 'total_tokens': 2341}\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(\"How to find your LM head details?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "The user is asking about my context length. I should guide him to check Qwen's official website.\n",
      "</think>\n",
      "\n",
      "For more information about me, please visit Qwen's official website.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "prompt engineering\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
